{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b5f28d-7d28-46b1-8807-443e6306d225",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283119b1-384a-4b47-84cd-115d3b67ae05",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This script allows us to combine all seasons CSV data, tokenize each line, specifically pre-processing by lowercasing lines and removing punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a3073-6e30-4e56-8d02-8b2c89fad9c4",
   "metadata": {},
   "source": [
    "First, we import all packages. This includes downloading the ```punkt``` package from the ```Natural Language ToolKit (nltk)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a8de22-0032-4b2e-88a2-d73871146c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52067106-b1e0-4842-8f53-98203a2246e7",
   "metadata": {},
   "source": [
    "Now, we can begin by combining all csvs from the data cleaning step into one large csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d81cc6-c819-4eed-8c7f-fcc1963552ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_csv = 'data/combined_seasons.csv'\n",
    "\n",
    "all_seasons = glob.glob(os.path.join(\"./data\", \"*.csv\"))\n",
    "\n",
    "df_csvs = [pd.read_csv(f) for f in all_seasons]\n",
    "combined_df = pd.concat(df_csvs, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(combined_csv, index=False)\n",
    "\n",
    "input_file = combined_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51153132-ca58-4741-ac5a-6b1ea487096c",
   "metadata": {},
   "source": [
    "For project purposes, there are counters created to determine the total number of tokens, lines, and vocab size for each character and overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbf85e8-b8d3-432e-93d2-7ee353c4e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = defaultdict(int)\n",
    "vocab = defaultdict(set)\n",
    "line_counts = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb08a92-0466-44e9-94b1-0a2e18e17e6b",
   "metadata": {},
   "source": [
    "The csv can then be opened, and each row in the CSV is iterated through, tokenized with the proper pre-processing as well. Then, for each character, the number of tokens, vocab size, and number of lines is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8894fdec-c091-4816-9b93-e6a29cf99370",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "\n",
    "    for row in reader:\n",
    "        character = row[\"character\"]\n",
    "        line = row[\"line\"]\n",
    "\n",
    "        tokens = word_tokenize(line.lower())\n",
    "\n",
    "        tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "        token_counts[character] += len(tokens)\n",
    "        vocab[character].update(tokens)\n",
    "        line_counts[character] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4187cd9-d3a9-48dd-83bf-bd1fd9eeafad",
   "metadata": {},
   "source": [
    "At the end, the data for each character is printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b93e909-35c5-4e15-b768-7d30253a3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex\n",
      "  Total Lines: 885\n",
      "  Total Tokens: 7278\n",
      "  Vocabulary Size: 1415\n",
      "\n",
      "Cameron\n",
      "  Total Lines: 2451\n",
      "  Total Tokens: 27850\n",
      "  Vocabulary Size: 3475\n",
      "\n",
      "Claire\n",
      "  Total Lines: 3433\n",
      "  Total Tokens: 34934\n",
      "  Vocabulary Size: 3087\n",
      "\n",
      "Gloria\n",
      "  Total Lines: 1907\n",
      "  Total Tokens: 20496\n",
      "  Vocabulary Size: 2257\n",
      "\n",
      "Haley\n",
      "  Total Lines: 1116\n",
      "  Total Tokens: 9716\n",
      "  Vocabulary Size: 1476\n",
      "\n",
      "Jay\n",
      "  Total Lines: 2473\n",
      "  Total Tokens: 29196\n",
      "  Vocabulary Size: 3143\n",
      "\n",
      "Luke\n",
      "  Total Lines: 960\n",
      "  Total Tokens: 7516\n",
      "  Vocabulary Size: 1414\n",
      "\n",
      "Manny\n",
      "  Total Lines: 1030\n",
      "  Total Tokens: 10034\n",
      "  Vocabulary Size: 1855\n",
      "\n",
      "Mitchell\n",
      "  Total Lines: 2752\n",
      "  Total Tokens: 29456\n",
      "  Vocabulary Size: 3079\n",
      "\n",
      "Phil\n",
      "  Total Lines: 3316\n",
      "  Total Tokens: 37590\n",
      "  Vocabulary Size: 4029\n",
      "\n",
      "Overall Statistics\n",
      "  Total Lines Overall: 20323\n",
      "  Total Tokens: 214066\n",
      "  Vocabulary Size: 9564\n"
     ]
    }
   ],
   "source": [
    "for character in sorted(token_counts.keys()):\n",
    "    total_tokens = token_counts[character]\n",
    "    vocab_size = len(vocab[character])\n",
    "\n",
    "    print(f\"{character}\")\n",
    "    print(f\"  Total Lines: {line_counts[character]}\")\n",
    "    print(f\"  Total Tokens: {total_tokens}\")\n",
    "    print(f\"  Vocabulary Size: {vocab_size}\")\n",
    "    print()\n",
    "\n",
    "unique_vocab_total = {word for char_vocab in vocab.values() for word in char_vocab}\n",
    "\n",
    "print(\"Overall Statistics\")\n",
    "print(f\"  Total Lines Overall: {sum(line_counts.values())}\")\n",
    "print(f\"  Total Tokens: {sum(token_counts.values())}\")\n",
    "print(f\"  Vocabulary Size: {len(unique_vocab_total)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5588aade-240e-45a8-b89b-f909995cb8ac",
   "metadata": {},
   "source": [
    "At this point, the tokenization and pre-processing is complete and the tokens are ready for following steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
